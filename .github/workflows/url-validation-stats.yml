name: URL Validation & Stats (Mon-Fri 9AM UTC)

# Validates job URLs for 404s and computes employer fill statistics
# Part of: EPIC-008 Curated Job Feed
#
# NOTE: Summary generation has moved inline to classifier.py
# The summary_generator.py script remains as a backfill utility only.

on:
  schedule:
    # Run at 9:00 AM UTC (after Greenhouse batch at 7:00 AM completes)
    - cron: '0 9 * * 1-5'  # Monday-Friday (weekday schedule)
  workflow_dispatch:
    inputs:
      url_limit:
        description: 'Limit for URL validation (blank = default 1000)'
        required: false
        default: ''
        type: string
      run_employer_stats:
        description: 'Run employer_stats.py after URL validation'
        required: false
        default: true
        type: boolean

jobs:
  url-validation-stats:
    runs-on: ubuntu-latest
    timeout-minutes: 45  # Increased for API freshness check + URL validation

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install chromium --with-deps

      # Step 1: API Freshness Check (runs before URL validation to reduce queue)
      - name: Run api_freshness_checker.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "=========================================="
          echo "STEP 1: API Freshness Check"
          echo "=========================================="
          python pipeline/api_freshness_checker.py

      # Step 2: URL Validation (must run before employer_stats)
      - name: Run url_validator.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "=========================================="
          echo "STEP 2: URL Validation"
          echo "=========================================="
          # Default limit of 1000 for scheduled runs (covers all 6 ATS sources)
          LIMIT="${{ github.event.inputs.url_limit }}"
          if [ -z "$LIMIT" ]; then
            LIMIT="1000"
          fi
          echo "Checking up to $LIMIT URLs..."
          python pipeline/url_validator.py --limit=$LIMIT

      # Step 3: Employer Stats (depends on 404 data from URL validation)
      - name: Run employer_stats.py
        if: ${{ github.event.inputs.run_employer_stats != 'false' }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "=========================================="
          echo "STEP 3: Employer Fill Stats"
          echo "=========================================="
          python pipeline/employer_stats.py

      - name: Summary
        if: always()
        run: |
          python << 'EOF'
          import os
          from datetime import datetime

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write("## URL Validation & Stats Complete\n\n")
              f.write(f"| Step | Status |\n")
              f.write(f"|------|--------|\n")
              f.write(f"| **API Freshness Check** | Complete |\n")
              f.write(f"| **URL Validator** | Complete |\n")
              f.write(f"| **Employer Stats** | Complete |\n")
              f.write(f"\n**Timestamp:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\n")
              f.write(f"\n_Note: Summary generation now happens inline during job scraping._\n")
          EOF
