name: Scrape Greenhouse (Mon/Tue/Thu/Fri 7AM UTC)

on:
  schedule:
    # Weekday-only schedule (jobs are posted Mon-Fri)
    # Staggered with Adzuna (Wed) to avoid Gemini rate limits
    - cron: '0 7 * * 1'  # Monday at 7:00 AM UTC (Batch 1)
    - cron: '0 7 * * 2'  # Tuesday at 7:00 AM UTC (Batch 2)
    - cron: '0 7 * * 4'  # Thursday at 7:00 AM UTC (Batch 3)
    - cron: '0 7 * * 5'  # Friday at 7:00 AM UTC (Batch 4)
  workflow_dispatch:
    inputs:
      batch_override:
        description: 'Force specific batch (1-4), or auto based on day'
        required: false
        default: 'auto'
        type: choice
        options:
          - 'auto'
          - '1'
          - '2'
          - '3'
          - '4'

jobs:
  greenhouse-batch:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # ~91 companies at ~47 sec each + API retry buffer

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('requirements.txt') }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps

      - name: Restore URL cache
        uses: actions/cache@v4
        with:
          path: output/greenhouse_scraper_cache.json
          key: greenhouse-url-cache-${{ github.run_id }}
          restore-keys: |
            greenhouse-url-cache-

      # URL overrides are now in config/greenhouse/company_ats_mapping.json (url_type field)
      # No seed step needed - scraper reads url_type from config directly

      - name: Compute batch and run scraper
        env:
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
          LLM_PROVIDER: gemini
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          BATCH_OVERRIDE: ${{ github.event.inputs.batch_override }}
        run: |
          python << 'EOF'
          import json
          import subprocess
          import os
          from datetime import datetime

          # Load companies from config
          with open('config/greenhouse/company_ats_mapping.json') as f:
              data = json.load(f)

          greenhouse = data.get('greenhouse', {})
          companies = sorted(greenhouse.keys())
          total_companies = len(companies)
          batch_size = total_companies // 4 + 1

          # Determine batch number (allow manual override)
          # Weekday schedule: Mon=Batch1, Tue=Batch2, Thu=Batch3, Fri=Batch4
          day_to_batch = {
              0: 0,  # Monday -> Batch 1 (index 0)
              1: 1,  # Tuesday -> Batch 2 (index 1)
              3: 2,  # Thursday -> Batch 3 (index 2)
              4: 3,  # Friday -> Batch 4 (index 3)
          }
          day_names = {0: 'Monday', 1: 'Tuesday', 3: 'Thursday', 4: 'Friday'}

          override = os.environ.get('BATCH_OVERRIDE', 'auto').strip()
          if override and override != 'auto':
              batch_num = int(override) - 1  # Convert 1-4 to 0-3
              print(f"Using manual override: Batch {override}")
          else:
              weekday = datetime.now().weekday()
              batch_num = day_to_batch.get(weekday, 0)  # Default to batch 1 if run on off-day
              day_name = day_names.get(weekday, f"Day {weekday}")
              print(f"Auto-detected: {day_name} -> Batch {batch_num + 1}")

          # Calculate batch slice
          start = batch_num * batch_size
          end = min(start + batch_size, total_companies)
          batch_companies = companies[start:end]

          # Get slugs for the batch
          slugs = [greenhouse[c]['slug'] for c in batch_companies]

          print(f"\n{'='*60}")
          print(f"GREENHOUSE BATCH {batch_num + 1}")
          print(f"{'='*60}")
          print(f"Companies: {len(slugs)} of {total_companies}")
          print(f"Range: {batch_companies[0]} -> {batch_companies[-1]}")
          print(f"Indices: [{start}:{end}]")
          print(f"{'='*60}\n")

          # Run the scraper
          result = subprocess.run([
              'python', 'wrappers/fetch_jobs.py',
              '--sources', 'greenhouse',
              '--companies', ','.join(slugs)
          ])

          exit(result.returncode)
          EOF

      - name: Upload URL cache as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: greenhouse-url-cache
          path: output/greenhouse_scraper_cache.json
          if-no-files-found: ignore

      - name: Summary
        if: always()
        env:
          BATCH_OVERRIDE: ${{ github.event.inputs.batch_override }}
        run: |
          python << 'EOF'
          import json
          import os
          from datetime import datetime

          with open('config/greenhouse/company_ats_mapping.json') as f:
              data = json.load(f)

          greenhouse = data.get('greenhouse', {})
          companies = sorted(greenhouse.keys())
          batch_size = len(companies) // 4 + 1

          # Weekday schedule: Mon=Batch1, Tue=Batch2, Thu=Batch3, Fri=Batch4
          day_to_batch = {0: 0, 1: 1, 3: 2, 4: 3}  # Mon/Tue/Thu/Fri

          override = os.environ.get('BATCH_OVERRIDE', 'auto').strip()
          if override and override != 'auto':
              batch_num = int(override) - 1
          else:
              batch_num = day_to_batch.get(datetime.now().weekday(), 0)

          start = batch_num * batch_size
          end = min(start + batch_size, len(companies))
          batch_companies = companies[start:end]

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
              f.write(f"## Greenhouse Batch {batch_num + 1} Complete\n\n")
              f.write(f"| Metric | Value |\n")
              f.write(f"|--------|-------|\n")
              f.write(f"| **Batch** | {batch_num + 1} of 4 |\n")
              f.write(f"| **Companies** | {len(batch_companies)} |\n")
              f.write(f"| **Range** | {batch_companies[0]} -> {batch_companies[-1]} |\n")
              f.write(f"| **Timestamp** | {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')} |\n")
          EOF
